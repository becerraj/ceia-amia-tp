{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3192d5ba",
   "metadata": {},
   "source": [
    "# Integrantes\n",
    "\n",
    "- Agustín Maglione  \n",
    "- Fernando Sebastián Lomazzi  \n",
    "- Juan Cruz Becerra  \n",
    "- Maria Gabriela Bohórquez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43349b",
   "metadata": {},
   "source": [
    "## Versiones de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.11 (main, Jun 26 2025, 21:20:05) [Clang 20.1.4 ]\n"
     ]
    }
   ],
   "source": [
    "# Versión de Python\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1dbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 2.3.1\n",
      "scipy 1.16.1\n",
      "scikit-learn 1.7.1\n",
      "tqdm 4.67.1\n"
     ]
    }
   ],
   "source": [
    "# Versiones de librerías principales\n",
    "import numpy, scipy, sklearn, tqdm\n",
    "print(\"numpy\", numpy.__version__)\n",
    "print(\"scipy\", scipy.__version__)\n",
    "print(\"scikit-learn\", sklearn.__version__)\n",
    "print(\"tqdm\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23656d1",
   "metadata": {},
   "source": [
    "# TP: LDA/QDA y optimización matemática de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d472e51",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción teórica (resumen)\n",
    "\n",
    "**Regla de Bayes.** Para clases con a priori $\\pi_j$ y densidades condicionales $f_j$, se decide\n",
    "$\n",
    "H(x)=\\arg\\max_j \\{\\log f_j(x)+\\log\\pi_j\\}.\n",
    "$\n",
    "\n",
    "**Modelo normal multivariado (QDA/LDA).**\n",
    "$\n",
    "\\log f_j(x)=-\\tfrac12\\log|\\Sigma_j|-\\tfrac12(x-\\mu_j)^\\top\\Sigma_j^{-1}(x-\\mu_j)+C.\n",
    "$\n",
    "En **LDA**, $\\Sigma_j=\\Sigma$ para todo $j$ y\n",
    "$\n",
    "\\log f_j(x)=\\mu_j^\\top\\Sigma^{-1}\\bigl(x-\\tfrac12\\mu_j\\bigr)+C'.\n",
    "$\n",
    "\n",
    "**Estimación (MV).** $\\hat\\mu_j=\\bar x_j$, $\\hat\\Sigma_j=s_j^2$, $\\hat\\pi_j=n_j/n$; en LDA, $\\hat\\Sigma=\\frac1n\\sum_j n_j s_j^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri\n",
    "\n",
    "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2d8a4",
   "metadata": {},
   "source": [
    "## Base de código (Profesor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseBayesianClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _estimate_a_priori(self, y):\n",
    "        a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "        # Q3: para qué sirve bincount?\n",
    "        return np.log(a_priori)\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # estimate all needed parameters for given model\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        # log P(x|G=class_idx) según el modelo\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit(self, X, y, a_priori=None):\n",
    "        # estimar probabilidades a priori si no se dan\n",
    "        self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "        # ajustar parámetros del modelo específico\n",
    "        self._fit_params(X, y)\n",
    "        # Q4: ¿por qué _fit_params va al final?\n",
    "\n",
    "    def predict(self, X):\n",
    "        # predicción individuo por individuo (loop externo)\n",
    "        m_obs = X.shape[1]\n",
    "        y_hat = np.empty(m_obs, dtype=int)\n",
    "        for i in range(m_obs):\n",
    "            y_hat[i] = self._predict_one(X[:, i].reshape(-1, 1))\n",
    "        # devolver predicción como vector fila\n",
    "        return y_hat.reshape(1, -1)\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # calcular log-posteriori para todas las clases\n",
    "        log_posteriori = [log_a_priori_i + self._predict_log_conditional(x, idx) \n",
    "                          for idx, log_a_priori_i in enumerate(self.log_a_priori)]\n",
    "        # devolver la clase con posteriori máxima\n",
    "        return np.argmax(log_posteriori)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f528153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_dataset():\n",
    "    data = load_iris()\n",
    "    X_full = data.data\n",
    "    y_full = data.target.reshape(-1, 1)\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_penguins_dataset():\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "    df.drop(columns=[\"island\", \"sex\"], inplace=True)\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "    return df.values, tgt.to_numpy().reshape(-1, 1)\n",
    "\n",
    "def get_wine_dataset():\n",
    "    data = load_wine()\n",
    "    X_full = data.data\n",
    "    y_full = data.target.reshape(-1, 1)\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_letters_dataset():\n",
    "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "    return letter.data, letter.target.reshape(-1, 1)\n",
    "\n",
    "def label_encode(y_full):\n",
    "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
    "\n",
    "def split_transpose(X, y, test_size, random_state):\n",
    "    # retornar X_train, X_test, y_train, y_test, todos transpuestos\n",
    "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdefb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy.random import RandomState\n",
    "import tracemalloc\n",
    "\n",
    "RNG_SEED = 6553\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self, X, y, n_runs=100, warmup=20, mem_runs=20, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = n_runs\n",
    "        self.warmup = warmup\n",
    "        self.mem_runs = mem_runs\n",
    "        self.test_sz = test_sz\n",
    "        self.det = same_splits\n",
    "        if self.det:\n",
    "            self.rng_seed = rng_seed\n",
    "        else:\n",
    "            self.rng = RandomState(rng_seed)\n",
    "\n",
    "        self.data = dict()\n",
    "        print(\"Benching params:\")\n",
    "        print(\"Total runs:\", self.warmup + self.mem_runs + self.n)\n",
    "        print(\"Warmup runs:\", self.warmup)\n",
    "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
    "        print(\"Running time runs:\", self.n)\n",
    "        approx_test_sz = int(self.y.size * self.test_sz)\n",
    "        print(\"Train size rows (approx):\", self.y.size - approx_test_sz)\n",
    "        print(\"Test size rows (approx):\", approx_test_sz)\n",
    "        print(\"Test size fraction:\", self.test_sz)\n",
    "\n",
    "    def bench(self, model_class, **kwargs):\n",
    "        name = model_class.__name__\n",
    "        time_data = np.empty((self.n, 3), dtype=float)    # [train_time, test_time, accuracy]\n",
    "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # [train_peak_mem, test_peak_mem]\n",
    "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
    "\n",
    "        # Warmup\n",
    "        for i in range(self.warmup):\n",
    "            model = model_class(**kwargs)\n",
    "            X_train, X_test, y_train, y_test = split_transpose(self.X, self.y,\n",
    "                                                               test_size=self.test_sz,\n",
    "                                                               random_state=rng)\n",
    "            model.fit(X_train, y_train)\n",
    "            model.predict(X_test)\n",
    "\n",
    "        # Mem runs\n",
    "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
    "            model = model_class(**kwargs)\n",
    "            X_train, X_test, y_train, y_test = split_transpose(self.X, self.y,\n",
    "                                                               test_size=self.test_sz,\n",
    "                                                               random_state=rng)\n",
    "            tracemalloc.start()\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            _, train_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.reset_peak()\n",
    "            model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            _, test_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            mem_data[i, :] = (train_peak / (1024 * 1024), test_peak / (1024 * 1024))\n",
    "\n",
    "        # Time runs\n",
    "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
    "            model = model_class(**kwargs)\n",
    "            X_train, X_test, y_train, y_test = split_transpose(self.X, self.y,\n",
    "                                                               test_size=self.test_sz,\n",
    "                                                               random_state=rng)\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            preds = model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            time_data[i, :] = ((t2 - t1) * 1000,\n",
    "                               (t3 - t2) * 1000,\n",
    "                               (y_test.flatten() == preds.flatten()).mean())\n",
    "\n",
    "        self.data[name] = (time_data, mem_data)\n",
    "\n",
    "    def summary(self, baseline=None):\n",
    "        aux = []\n",
    "        for name, (time_data, mem_data) in self.data.items():\n",
    "            result = {\n",
    "                'model': name,\n",
    "                'train_median_ms': np.median(time_data[:, 0]),\n",
    "                'train_std_ms': time_data[:, 0].std(),\n",
    "                'test_median_ms': np.median(time_data[:, 1]),\n",
    "                'test_std_ms': time_data[:, 1].std(),\n",
    "                'mean_accuracy': time_data[:, 2].mean(),\n",
    "                'train_mem_median_mb': np.median(mem_data[:, 0]),\n",
    "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
    "                'test_mem_median_mb': np.median(mem_data[:, 1]),\n",
    "                'test_mem_std_mb': mem_data[:, 1].std()\n",
    "            }\n",
    "            aux.append(result)\n",
    "        df = pd.DataFrame(aux).set_index('model')\n",
    "        if baseline is not None and baseline in self.data:\n",
    "            df['train_speedup'] = df.loc[baseline, 'train_median_ms'] / df['train_median_ms']\n",
    "            df['test_speedup'] = df.loc[baseline, 'test_median_ms'] / df['test_median_ms']\n",
    "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_median_mb'] / df['train_mem_median_mb']\n",
    "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_median_mb'] / df['test_mem_median_mb']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_implementaciones(\n",
    "    get_datasets_fn=None,\n",
    "    models=None,\n",
    "    n_runs=10, warmup=3, mem_runs=3, test_sz=0.3, same_splits=False, baseline=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta Benchmark sobre un conjunto de modelos y devuelve/imprime el resumen.\n",
    "    - get_dataset_fn: función que retorna (X_full, y_full)\n",
    "    - models: lista de clases de modelo a evaluar.\n",
    "    - *_runs: cantidad de corridas para warmup/mem/time; test_sz: fracción test;\n",
    "    - same_splits: si True usa splits deterministas; baseline: nombre para speedup/reducción.\n",
    "    \"\"\"\n",
    "    if get_datasets_fn is None:\n",
    "        print(\"No se especificó get_dataset_fn\")\n",
    "        return\n",
    "    if baseline is None:\n",
    "        print(\"No se especificó baseline sobre el cual comparar los modelos\")\n",
    "        return\n",
    "    if not isinstance(baseline, str):\n",
    "        print(\"El parametro baseline debe ser un string con el nombre exacto del modelo\")\n",
    "        return\n",
    "    results = {}\n",
    "    for get_dataset in get_datasets_fn:\n",
    "        X_full, y_full = get_dataset()\n",
    "        y_enc = label_encode(y_full)\n",
    "        b = Benchmark(X_full, y_enc, n_runs=n_runs, warmup=warmup, mem_runs=mem_runs,\n",
    "                    test_sz=test_sz, same_splits=same_splits)\n",
    "        if models is None:\n",
    "            print(\"No se especificaron modelos\")\n",
    "            return\n",
    "        for model in models:\n",
    "            b.bench(model)\n",
    "        df = b.summary(baseline=baseline)\n",
    "        print(get_dataset.__name__, \": \\n\", df)\n",
    "        results[get_dataset.__name__] = df\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ed110",
   "metadata": {},
   "source": [
    "# Respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0aa9e",
   "metadata": {},
   "source": [
    "# 1) Tensorización — Incisos 1 a 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # estimar matriz de covarianzas inversa para cada clase\n",
    "        self.inv_covs = [LA.inv(np.cov(X[:, y.flatten() == idx], bias=True))\n",
    "                         for idx in range(len(self.log_a_priori))]\n",
    "        # medias por clase\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        inv_cov = self.inv_covs[class_idx]\n",
    "        unbiased_x = x - self.means[class_idx]\n",
    "        return 0.5 * np.log(LA.det(inv_cov)) - 0.5 * (unbiased_x.T @ inv_cov @ unbiased_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cc3af",
   "metadata": {},
   "source": [
    "## Inciso 1 — ¿Sobre qué paraleliza `TensorizedQDA`?\n",
    "\n",
    "**Respuesta:** Paraleliza sobre las $k$ clases (no sobre las $n$ observaciones). Para cada $x$ calcula $\\log f_j(x)$ para todos los $j$ en una sola operación vectorizada; el loop sobre observaciones se mantiene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "        # tensores apilados para vectorizar sobre clases\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)   # (k,p,p)\n",
    "        self.tensor_means   = np.stack(self.means)      # (k,p,1)\n",
    "\n",
    "    def _predict_log_conditionals(self, x):\n",
    "        unbiased_x = x - self.tensor_means                          # (k,p,1)\n",
    "        inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x  # (k,1,1)\n",
    "        return 0.5 * np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d3be1",
   "metadata": {},
   "source": [
    "## Inciso 2 — Shapes y equivalencia entre `QDA` y `TensorizedQDA`\n",
    "\n",
    "`tensor_inv_cov: (k,p,p)` y `tensor_means: (k,p,1)`. \n",
    "\n",
    "Para un $x\\in\\mathbb{R}^{p\\times 1}$, `unbiased_x = x - tensor_means` da $(k,p,1)$; \n",
    "\n",
    "el producto `unbiased_x^T @ tensor_inv_cov @ unbiased_x` produce $(k,1,1)$ con los valores $(x-\\mu_j)^\\top\\Sigma_j^{-1}(x-\\mu_j)$. \n",
    "\n",
    "Sumando $\\log\\pi_j$ se obtiene la misma predicción que `QDA`.\n",
    "\n",
    "\n",
    "**Detalle de shapes (paso a paso)**  \n",
    "- `x`: `(p, 1)`  \n",
    "- `tensor_means`: `(k, p, 1)`  ⇒ `unbiased_x = x - tensor_means` ⇒ `(k, p, 1)`  \n",
    "- `tensor_inv_cov`: `(k, p, p)`  \n",
    "- `unbiased_x.transpose(0, 2, 1) @ tensor_inv_cov @ unbiased_x` ⇒ `(k, 1, 1)`  \n",
    "- Aplanando, obtenemos `(k,)` con una log-verosimilitud por clase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce292712",
   "metadata": {},
   "source": [
    "# 2) Optimización - Incisos 3 a 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99cbba",
   "metadata": {},
   "source": [
    "## Inciso 3 — Implementación `FasterQDA` (sin loops, pero ineficiente en memoria)\n",
    "\n",
    "Vectoriza también sobre observaciones formando, para cada clase, la matriz $(X-\\mu)^\\top\\Sigma^{-1}(X-\\mu)$ de tamaño $n\\times n$ y tomando su diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac842545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FasterQDA(TensorizedQDA):\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "        # Precompute 0.5*log|Σ_k^{-1}| = -0.5*log|Σ_k|\n",
    "        # Usamos self.tensor_inv_cov que es (K,p,p)\n",
    "        self.half_logdet_inv = 0.5 * np.log(np.linalg.det(self.tensor_inv_cov))  # (K,)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice en batch sin bucles formando la matriz (n x n) por clase:\n",
    "        X: (p, m) -> retorna (1, m)\n",
    "        \"\"\"\n",
    "        # Dif de cada clase con cada columna de X: (K,p,m)\n",
    "        diff = X[None, :, :] - self.tensor_means      # (K,p,1) broadcast -> (K,p,m)\n",
    "\n",
    "        # Matmul batcheado sobre clases: (K,p,p) @ (K,p,m) -> (K,p,m)\n",
    "        y = self.tensor_inv_cov @ diff\n",
    "\n",
    "        # Matriz completa Q_full = (X-μ)^T Σ^{-1} (X-μ) de tamaño (m x m) por clase\n",
    "        Q_full = diff.transpose(0, 2, 1) @ y   # (K,m,m)\n",
    "\n",
    "        # Diagonal (una cuadrática por observación)\n",
    "        quad = np.diagonal(Q_full, axis1=1, axis2=2)     # (K,m)\n",
    "\n",
    "        # Puntajes g_k(x) = log π_k + 0.5*log|Σ_k^{-1}| - 0.5*quad\n",
    "        scores = self.log_a_priori[:, None] + self.half_logdet_inv[:, None] - 0.5 * quad  # (K,m)\n",
    "\n",
    "        # Clase argmax por columna\n",
    "        return scores.argmax(axis=0, keepdims=True)   # (1,m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a7784",
   "metadata": {},
   "source": [
    "## Inciso 4 — ¿Dónde aparece la matriz $n\\times n$? ¿Cómo evitarla?\n",
    "\n",
    "Para una clase fija $k$, sea:\n",
    "\n",
    "- $D_k = X - μ_k$ con $X$ $(p\\times n)$ y $μ_k$ $(p\\times 1)$ (broadcast) $→ D_k$ es $(p\\times n)$.\n",
    "\n",
    "- Cuadrática por observación: queremos el vector $d(x_i) = (x_i−μ_k)^T Σ_k^{-1} (x_i−μ_k)$ de tamaño $n$.\n",
    "\n",
    "Si hacemos la vectorización con `@`:\n",
    "\n",
    "$Q_{kfull} = D_k.T$ @ $Σ_k^{-1}$ @ $D_k →$ matriz $(n\\times n)$.\n",
    "\n",
    "Cada entrada $(i,j) = (x_i−μ_k)^T Σ_k^{-1} (x_j−μ_k)$.\n",
    "\n",
    "Lo que necesitamos son solo las diagonales: $\\operatorname{diag}(Q_{kfull}) = d(x_i)$.\n",
    "\n",
    "Esto muestra explícitamente la matriz $(n\\times n)$.\n",
    "\n",
    "---\n",
    "\n",
    "Cómo lo evitamos en `FasterQDA`:\n",
    "\n",
    "Implementamos:\n",
    "- $Y_k = Σ_k^{-1} @ D_k$ $(p×n)$,\n",
    "- $d = (D_k * Y_k).sum(axis=1)$ (vector $n$),\n",
    "\n",
    "Sin nunca formar $Q_{kfull}$ $(n×n)$.\n",
    "\n",
    "En el código anterior, esto es exactamente:\n",
    "\n",
    "`y = tensor_inv_cov @ diff` y luego `quad = (diff * y).sum(axis=1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadd7e7",
   "metadata": {},
   "source": [
    "## Inciso 5 - Demostración \n",
    "\n",
    "**Prueba por componentes (índice a índice).** \n",
    "\n",
    "La entrada $(i,i)$ de $AB$ es $(AB)_{ii}=\\sum_{k=1}^p A_{ik}B_{ki}$. \n",
    "\n",
    "Como $(B^\\top)_{ik}=B_{ki}$, se tiene $(A⊙B^\\top)_{ik}=A_{ik}(B^\\top)_{ik}=A_{ik}B_{ki}$. \n",
    "\n",
    "Al sumar por columnas (en $k$) la fila $i$ de $A⊙B^\\top$, obtenemos $\\sum_{k=1}^p (A⊙B^\\top)_{ik}=(AB)_{ii}$. \n",
    "\n",
    "Por lo tanto, $diag⁡(AB)=\\sum_{\\text{cols}}(A\\odot B^\\top)=np.sum(A \\odot B.T, axis=1)$.\n",
    "\n",
    "---\n",
    "\n",
    "**Por qué “esquivamos” la matriz $n\\times n$.**\n",
    "\n",
    "Formar $AB$ completo para luego tomar solo la diagonal requiere memoria $O(n^2)$ y tiempo $O(n^2p)$. \n",
    "\n",
    "En cambio, calcular directamente $np.sum(A \\odot B.T, axis=1)$ nunca construye la matriz $n\\times n$, usa solo objetos $n\\times p$ (más un vector) y añade $O(np)$ operaciones, habilitando la versión eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86320a90",
   "metadata": {},
   "source": [
    "## Inciso 6 - Propiedad clave (para `EfficientQDA`) — sin formar la matriz $n\\times n$\n",
    "\n",
    "Para **evitar la matriz $n\\times n$**, usamos la identidad\n",
    "$\\operatorname{diag}(AB)=\\sum (A\\odot B^T)$ y obtenemos directamente la diagonal con operaciones elemento a elemento. \n",
    "\n",
    "Implementado en `EfficientQDA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EfficientQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "        # Vectorizar sobre clases y observaciones sin formar (n x n)\n",
    "        unbiased_X = X[np.newaxis, :, :] - self.tensor_means   # (k,p,n)\n",
    "        B = self.tensor_inv_cov @ unbiased_X         # (k,p,n)\n",
    "        cond_quad = np.sum(unbiased_X * B, axis=1)             # (k,n)  diag((X-mu)^T Sig^-1 (X-mu))\n",
    "        log_det_terms = 0.5 * np.log(np.linalg.det(self.tensor_inv_cov))  # (k,)\n",
    "        log_cond_matrix = log_det_terms[:, None] - 0.5 * cond_quad        # (k,n)\n",
    "        log_post = log_cond_matrix + self.log_a_priori[:, None]            # (k,n)\n",
    "        return np.argmax(log_post, axis=0).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d035ca0",
   "metadata": {},
   "source": [
    "## Inciso 7 - Comparacion QDA (en sus 4 versiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b00111",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion_4_variantes = comparar_implementaciones(\n",
    "        get_dataset_fn=[get_iris_dataset, get_wine_dataset, get_penguins_dataset, get_letters_dataset],\n",
    "        models=[QDA, TensorizedQDA, FasterQDA, EfficientQDA],\n",
    "        n_runs=5, warmup=2, mem_runs=2, test_sz=0.3, same_splits=False, baseline='QDA'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bffafd",
   "metadata": {},
   "source": [
    "# 3) Cholesky (Comparaciones) — Incisos 8 a 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81644087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TensorizedChol(BaseBayesianClassifier):\n",
    "    \"\"\"QDA con Cholesky vectorizado sobre observaciones.\n",
    "    - Ajuste: guarda L_k y mu_k por clase, y suma de log(diag(L_k)).\n",
    "    - Predicción: resuelve L_k Y = (X - mu_k) para TODAS las columnas de X.\n",
    "    \"\"\"\n",
    "    def _fit_params(self, X, y):\n",
    "        self.Ls = [\n",
    "            cholesky(np.cov(X[:, y.flatten() == idx], bias=True), lower=True)\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "        self.sumlog_diagL = np.array([np.log(L.diagonal()).sum() for L in self.Ls])\n",
    "\n",
    "    def predict(self, X):\n",
    "        k, n = len(self.log_a_priori), X.shape[1]\n",
    "        log_cond = np.empty((k, n))\n",
    "        for idx, L in enumerate(self.Ls):\n",
    "            unbiased_X = X - self.means[idx]            # (p,n)\n",
    "            Y = solve_triangular(L, unbiased_X, lower=True)  # (p,n)\n",
    "            quad = (Y ** 2).sum(axis=0)                 # (n,)\n",
    "            log_cond[idx, :] = -self.sumlog_diagL[idx] - 0.5 * quad\n",
    "        log_post = log_cond + self.log_a_priori[:, None]\n",
    "        return np.argmax(log_post, axis=0).reshape(1, -1)\n",
    "\n",
    "\n",
    "class EfficientChol(BaseBayesianClassifier):\n",
    "    \"\"\"QDA con Cholesky + matriz triangular invertida precomputada (vectorizado).\n",
    "    - Ajuste: L_k^{-1} por clase vía LAPACK (dtrtri), y mu_k.\n",
    "    - Predicción: Y = L_k^{-1} (X - mu_k) para TODAS las columnas (producto matricial).\n",
    "    \"\"\"\n",
    "    def _fit_params(self, X, y):\n",
    "        self.L_invs = [\n",
    "            dtrtri(cholesky(np.cov(X[:, y.flatten() == idx], bias=True), lower=True), lower=1)[0]\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "        # log |Σ|^{-1/2} = - sum log diag(L) = sum log diag(L^{-1})\n",
    "        self.sumlog_diagLinv = np.array([np.log(Li.diagonal()).sum() for Li in self.L_invs])\n",
    "\n",
    "    def predict(self, X):\n",
    "        k, n = len(self.log_a_priori), X.shape[1]\n",
    "        log_cond = np.empty((k, n))\n",
    "        for idx, L_inv in enumerate(self.L_invs):\n",
    "            unbiased_X = X - self.means[idx]            # (p,n)\n",
    "            Y = L_inv @ unbiased_X                      # (p,n)\n",
    "            quad = (Y ** 2).sum(axis=0)                 # (n,)\n",
    "            log_cond[idx, :] = self.sumlog_diagLinv[idx] - 0.5 * quad\n",
    "        log_post = log_cond + self.log_a_priori[:, None]\n",
    "        return np.argmax(log_post, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7570592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        self.L_invs = [\n",
    "            LA.inv(cholesky(np.cov(X[:, y.flatten() == idx], bias=True), lower=True))\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        L_inv = self.L_invs[class_idx]\n",
    "        unbiased_x = x - self.means[class_idx]\n",
    "        y = L_inv @ unbiased_x\n",
    "        return np.log(L_inv.diagonal().prod()) - 0.5 * (y ** 2).sum()\n",
    "\n",
    "\n",
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        self.Ls = [\n",
    "            cholesky(np.cov(X[:, y.flatten() == idx], bias=True), lower=True)\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        L = self.Ls[class_idx]\n",
    "        unbiased_x = x - self.means[class_idx]\n",
    "        y = solve_triangular(L, unbiased_x, lower=True)\n",
    "        return -np.log(L.diagonal().prod()) - 0.5 * (y ** 2).sum()\n",
    "\n",
    "\n",
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        self.L_invs = [\n",
    "            dtrtri(cholesky(np.cov(X[:, y.flatten() == idx], bias=True), lower=True), lower=1)[0]\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        L_inv = self.L_invs[class_idx]\n",
    "        unbiased_x = x - self.means[class_idx]\n",
    "        y = L_inv @ unbiased_x\n",
    "        return np.log(L_inv.diagonal().prod()) - 0.5 * (y ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb333bb",
   "metadata": {},
   "source": [
    "## Inciso 8 — Si $A=LL^\\top$, expresar $A^{-1}$ y utilidad en QDA\n",
    "\n",
    "$A^{-1}=(L^\\top)^{-1}L^{-1}$. \n",
    "\n",
    "Entonces $(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)=\\|L^{-1}(x-\\mu)\\|^2$ y $\\tfrac12\\log|\\Sigma^{-1}|=\\log|L^{-1}|$. \n",
    "\n",
    "La utilidad en QDA radica en que permite evitar invertir $\\Sigma$ y resolver sistemas triangulares más estables y rápidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813c2bf",
   "metadata": {},
   "source": [
    "## Inciso 9 — Diferencias `QDA` vs `QDA_Chol1` y paso a paso de predicción\n",
    "#### Entrenamiento:\n",
    "\n",
    "- `QDA`:\n",
    "    - Estima $Σ_j$ con `np.cov(…, bias=True)` y guarda su inversa completa: `inv_covs[j] = LA.inv(Σ_j)`.\n",
    "    - Guarda medias: `means[j]`.\n",
    "- `QDA_Chol1`:\n",
    "    - Factoriza $Σ_j = L_j L_j^T$ con `cholesky(Σ_j, lower=True)` y guarda la inversa triangular: `L_invs[j] = LA.inv(L_j)`.\n",
    "    - Guarda medias: `means[j]`.\n",
    "\n",
    "#### Predicción (paso a paso) en `QDA_Chol1._predict_log_conditional(x, j)`:\n",
    "1. Centrado: `unbiased_x = x − means[j]`.\n",
    "2. Blanqueo triangular: `y = L_invs[j] @ unbiased_x` (equivalente a resolver $L_j$ `y = unbiased_x`).\n",
    "3. Forma cuadrática: `-0.5 ||y||^2 = -0.5 * (y**2).sum()`, que es $-0.5 (x − μ)^T Σ_j^{-1} (x − μ)$.\n",
    "4. Log-det: `np.log(L_inv.diagonal().prod())`. Para triangular, $diag(L_inv) = 1/diag(L)$, por lo que este término vale $−∑ log diag(L) = 0.5 log|Σ_j^{-1}|$.\n",
    "5. Log-condicional: suma de (4) y (3).\n",
    "6. Posterior y clase: se suma `log_a_priori[j]` y se toma `argmax` (heredado de `BaseBayesianClassifier._predict_one`).\n",
    "\n",
    "#### Resumen de diferencia clave:\n",
    "- `QDA` usa la inversa completa $Σ_j^{-1}$ y el determinante de esa inversa.\n",
    "- `QDA_Chol1` evita invertir $Σ_j$ completa: trabaja con la inversa triangular $L_j^{-1}$ y productos/diagonales de matrices triangulares (mejor condicionado que invertir $Σ$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf52891",
   "metadata": {},
   "source": [
    "## Inciso 10 — Diferencias entre QDA_Chol1, QDA_Chol2 y QDA_Chol3\n",
    "\n",
    "#### Qué guarda cada uno (fit):\n",
    "- `QDA_Chol1`: guarda $L_j^{-1}$ (“inversa triangular”) tras Cholesky. Lo hace usando `L_invs = [LA.inv(cholesky(...))]`.\n",
    "- `QDA_Chol2`: guarda $L_j$ (no su inversa). Lo hace usando `Ls = [cholesky(...)]`.\n",
    "- `QDA_Chol3`: guarda $L_j^{-1}$ usando LAPACK especializado. Lo hace usando `L_invs = [dtrtri(cholesky(...), lower=1)[0]]`.\n",
    "\n",
    "#### Cómo predicen:\n",
    "- `QDA_Chol1`: $y = L_j^{-1}(x − μ_j)$; $log-det = log(∏ diag(L_j^{-1}))$; $log-cond = log-det − 0.5||y||^2$.\n",
    "- `QDA_Chol2`: resuelve $L_j y = (x − μ_j)$ con solve_triangular (sin invertir); $log-det = −log(∏ diag(L_j))$; $log-cond = log-det − 0.5||y||^2$.\n",
    "- `QDA_Chol3`: igual a Chol1, pero obtiene $L_j^{-1}$ de forma más eficiente/estable con dtrtri (mejor que `LA.inv` general para triangulares).\n",
    "\n",
    "#### Estabilidad numérica:\n",
    "- Mejor: `QDA_Chol2` (no hay inversión; solo solve triangular).\n",
    "- Intermedia: `QDA_Chol3` (inversión triangular con LAPACK; más estable/rápida que invertir una matriz densa).\n",
    "- Peor de los tres: `QDA_Chol1` (inversión triangular con `LA.inv`, aun así bastante mejor que invertir la matriz $Σ$ completa).\n",
    "\n",
    "#### Costos (orden aproximado):\n",
    "- Cholesky por clase: $O(p^3)$ (común a los tres).\n",
    "- Invertir triangular (`Chol1`/`Chol3`): $O(p^3)$ adicional al fit; predicción luego es $O(p^2)$ por observación (multiplicar $L^{-1} @ v$).\n",
    "- Resolver triangular (`Chol2`): $O(p^2)$ por observación sin costo de inversión; suele ser la opción más estable y competitiva en tiempo.\n",
    "\n",
    "#### Log-det:\n",
    "- `Chol1`/`Chol3`: usan $log(∏ diag(L^{-1})) = −∑ log diag(L) = 0.5 log|Σ^{-1}|$.\n",
    "- `Chol2`: computa directamente $−log(∏ diag(L)) = −∑ log diag(L)$, que es el mismo valor.\n",
    "\n",
    "#### Cuándo preferir:\n",
    "- En casos de muchas predicciones por cada ajuste y $p$ no muy grande: precomputar $L^{-1}$ (`Chol3` > `Chol1`) puede acelerar por evitar múltiples “solves”.\n",
    "- En casos de estabilidad prioritaria y/o $p$ moderado/grande: `Chol2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30f6f6",
   "metadata": {},
   "source": [
    "## Inciso 11 — Comparación breve de performance esperada\n",
    "\n",
    "Esta sección agrega una función utilitaria que corre el Benchmark sobre las variantes de QDA\n",
    "(`QDA`, `TensorizedQDA`, `QDA_Chol1`, `QDA_Chol2`, `QDA_Chol3`) y muestra un resumen.\n",
    "\n",
    "En *fit*, `Chol2` suele ser más rápido (no invierte). \n",
    "\n",
    "En *predict*, las tres son similares ($O(p^2)$ por observación). \n",
    "\n",
    "En datasets con $p$ grande o covarianzas mal condicionadas, `Chol2` es preferible por estabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cde49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar comparación rápida (Iris) y mostrar resumen al final del notebook\n",
    "comparacion_7_variantes = comparar_implementaciones(\n",
    "        get_dataset_fn=[get_iris_dataset, get_penguins_dataset, get_wine_dataset, get_letters_dataset],\n",
    "        models=[QDA, TensorizedQDA, FasterQDA, EfficientQDA, QDA_Chol1, QDA_Chol2, QDA_Chol3],\n",
    "        n_runs=5, warmup=2, mem_runs=2, test_sz=0.3, same_splits=False, baseline='QDA'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60828527",
   "metadata": {},
   "source": [
    "# 4) Cholesky (Optimización) — Incisos 12 a 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d1f09",
   "metadata": {},
   "source": [
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
    "14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa948b9",
   "metadata": {},
   "source": [
    "## Inciso 12 - Modelo `TensorizedChol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ac816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedChol(QDA_Chol2):\n",
    "    \"\"\"QDA con Cholesky heredando de QDA_Chol2.\n",
    "    - Ajuste: usa L_k y mu_k de QDA_Chol2 y precomputa sum(log diag L_k).\n",
    "    - Predicción: vectorizado sobre observaciones (resuelve L_k Y = X - mu_k por clase).\n",
    "    \"\"\"\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "        # 0.5*log|Sigma^{-1}| = -sum log diag(L)  (porque |Sigma| = (prod diag L)^2)\n",
    "        self.sumlog_diagL = np.array([np.log(L.diagonal()).sum() for L in self.Ls])\n",
    "\n",
    "    def predict(self, X):\n",
    "        k, n = len(self.log_a_priori), X.shape[1]\n",
    "        log_cond = np.empty((k, n))\n",
    "        for idx, L in enumerate(self.Ls):\n",
    "            unbiased_X = X - self.means[idx]                      # (p,n)\n",
    "            # resolver L Y = unbiased_X sin formar inversas\n",
    "            Y = solve_triangular(L, unbiased_X, lower=True, check_finite=False)\n",
    "            quad = (Y ** 2).sum(axis=0)                           # (n,)\n",
    "            log_cond[idx, :] = -self.sumlog_diagL[idx] - 0.5 * quad\n",
    "        log_post = log_cond + self.log_a_priori[:, None]\n",
    "        return np.argmax(log_post, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9e0d6",
   "metadata": {},
   "source": [
    "## Inciso 13 - Modelo `EfficientChol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientChol(TensorizedChol):\n",
    "    \"\"\"QDA con Cholesky eficiente heredando de TensorizedChol.\n",
    "    - Combina ideas de EfficientQDA (sin n×n) + tensorización sobre clases.\n",
    "    - Ajuste: computa L_k^{-1} (vía dtrtri) y apila tensores para vectorizar (k,p,p).\n",
    "    - Predicción: Y = L_k^{-1} (X - mu_k) vectorizado sobre clases y observaciones.\n",
    "    \"\"\"\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)  # provee self.Ls, self.means, self.sumlog_diagL\n",
    "        # Inversa triangular inferior por clase (más estable/eficiente que invertir Sigma)\n",
    "        self.L_invs = [dtrtri(L, lower=1, overwrite_c=0)[0] for L in self.Ls]\n",
    "        # Tensores apilados para vectorizar sobre clases\n",
    "        self.tensor_L_inv = np.stack(self.L_invs)   # (k,p,p)\n",
    "        self.tensor_means = np.stack(self.means)    # (k,p,1)\n",
    "        # 0.5*log|Sigma^{-1}| = sum log diag(L^{-1})\n",
    "        self.sumlog_diagLinv = np.array([np.log(Li.diagonal()).sum() for Li in self.L_invs])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Vectorizamos sobre clases (k) y observaciones (n) sin formar (n×n)\n",
    "        unbiased_X = X[np.newaxis, :, :] - self.tensor_means     # (k,p,n)\n",
    "        Y = self.tensor_L_inv @ unbiased_X                        # (k,p,n)\n",
    "        cond_quad = (Y ** 2).sum(axis=1)                          # (k,n)\n",
    "        log_post = (self.log_a_priori[:, None]\n",
    "                    + self.sumlog_diagLinv[:, None]\n",
    "                    - 0.5 * cond_quad)                           # (k,n)\n",
    "        return np.argmax(log_post, axis=0).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c417e3",
   "metadata": {},
   "source": [
    "## Inciso 14 - Performance de las 9 variantes de QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a575061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar comparación rápida (Iris) y mostrar resumen al final del notebook\n",
    "comparacion_9_variantes = comparar_implementaciones(\n",
    "        get_dataset_fn=[get_iris_dataset, get_penguins_dataset, get_wine_dataset, get_letters_dataset],\n",
    "        models=[QDA, TensorizedQDA, FasterQDA, EfficientQDA, QDA_Chol1, QDA_Chol2, QDA_Chol3, TensorizedChol, EfficientChol],\n",
    "        n_runs=5, warmup=2, mem_runs=2, test_sz=0.3, same_splits=False, baseline='QDA'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisisMatematico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
